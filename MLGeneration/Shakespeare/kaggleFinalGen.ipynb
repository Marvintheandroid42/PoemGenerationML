{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-29T00:24:18.322953Z","iopub.execute_input":"2021-12-29T00:24:18.323268Z","iopub.status.idle":"2021-12-29T00:24:18.333634Z","shell.execute_reply.started":"2021-12-29T00:24:18.32323Z","shell.execute_reply":"2021-12-29T00:24:18.332738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow._api.v2 import data\nimport random\n\nfilePath = tf.keras.utils.get_file(\n    \"shakespeare.txt\", \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n\ntext = open(filePath, 'rb').read().decode(encoding='utf-8')\n\nvocab = sorted(set(text))\n\n# charectar to integer representation (unqiue charecters and integer represetnation dictionary)\nchar2idx = {u: i for i, u in enumerate(vocab)}\n# integer to charectar represetnation\nidx2char = np.array(vocab)\n# look up the idx representation of the charectars in the text and store it as an array of idx reprentations\ntextAsInt = np.array([char2idx[char] for char in text])\n\nseqLength = 100\nexamplesPerEpoch = len(text)//(seqLength+1)\n\n# split idx representations into indidvidual slices\ncharDataset = tf.data.Dataset.from_tensor_slices(textAsInt)\n\n# Preparing the batches of data\nsequences = charDataset.batch(seqLength+1, drop_remainder=True)\n\n# splitting the labels and the input data\n\n\ndef splitInputTarget(chunk):\n    inputText = chunk[:-1]\n    targetText = chunk[1:]\n    return inputText, targetText\n\n\n# repreating the splitting function for all the sequences in the dataset\ndataset = sequences.map(splitInputTarget)\n\nBATCH_SIZE = 128\nBUFFER_SIZE = 10000\n\n# Prepare the batches and randomizing it\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\nvocabSize = len(vocab)\nprint(\"VOCABSIZE =\", vocabSize)\nembeddingDim = 256\nrnnUnits = 1024\n\n\ndef buildModel(vocabSize, embeddingDim, rnnUnits, batchSize):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(\n            vocabSize, embeddingDim, batch_input_shape=[batchSize, None]),\n        tf.keras.layers.GRU(rnnUnits, return_sequences=True,\n                            stateful=True, recurrent_initializer=\"glorot_uniform\"),\n        tf.keras.layers.LSTM(rnnUnits, return_sequences=True,\n                            stateful=True, recurrent_initializer=\"glorot_uniform\"),\n        tf.keras.layers.LSTM(rnnUnits, return_sequences=True,\n                            stateful=True, recurrent_initializer=\"glorot_uniform\"),\n        tf.keras.layers.GRU(rnnUnits, return_sequences=True,\n                            stateful=True, recurrent_initializer=\"glorot_uniform\"),\n        tf.keras.layers.Dense(vocabSize)\n    ])\n    return model\n\n\nmodel = buildModel(vocabSize=len(vocab), embeddingDim=embeddingDim,\n                   rnnUnits=rnnUnits, batchSize=BATCH_SIZE)\n\n# Loss Function\n\n\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\nmodel.compile(optimizer=optimizer, loss=loss,  metrics=[\"accuracy\"])\ncheckpointDir = \"./kaggle/working/training_checkpoint_Shakspeare_OptimizerChangedKaggle\"\ncheckpointPrefix = os.path.join(checkpointDir, \"chkpt_{epoch}\")\ncheckpointCallback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpointPrefix, save_weights_only=True, monitor='val_accuracy')\n\nEPOCHS = 50\n\n# TRAIN\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpointCallback])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-29T11:19:28.43651Z","iopub.execute_input":"2021-12-29T11:19:28.436925Z","iopub.status.idle":"2021-12-29T11:50:50.571117Z","shell.execute_reply.started":"2021-12-29T11:19:28.436889Z","shell.execute_reply":"2021-12-29T11:50:50.570317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# REBUILD FROM EXISTING TRAINING\nmodel = buildModel(vocabSize, embeddingDim, rnnUnits, batchSize=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpointDir))\n\nmodel.build(tf.TensorShape([1, None]))\n\nmodel.summary()\n\n# generating text\n\n\ndef generateText(model, startString): \n    # of charectars generated\n    numGenerate = 20000\n    # convert start string into idx\n    inputRep = [char2idx[s] for s in startString]\n    inputRep = tf.expand_dims(inputRep, 0)\n\n    txtGenerated = []\n    # Handles randomness through a scale factor (smaller means predictable(increase for more randomness))\n    tempreture = 1.0\n    model.reset_states()\n    for i in range(numGenerate):\n        predictions = model(inputRep)\n        predictions = tf.squeeze(predictions, 0)\n        predictions = predictions/tempreture\n        predictedID = tf.random.categorical(\n            predictions, num_samples=1)[-1, 0].numpy()\n\n        inputRep = tf.expand_dims([predictedID], 0)\n        txtGenerated.append(idx2char[predictedID])\n    return (startString + \"\".join(txtGenerated))\n\n# prompts = [\"ROMEO: \", \"PUCK: \", \"JULIET: \", \"IAGO: \",\n# \"LEAR: \", \"HAMLET: \", \"MACBETH:\", \"hath \", \"thou \"]\n\nprint(\"PASSED\")\n\nword = [\"Hath\", \"Thou\", \"Romeo\", \"Hamlet\", \"Macbeth\", \"Iago\",\"Hath\", \"Young\" ]\nfileOut = open(\"./kaggle/working/ShakspeareGen4.txt\", \"w\")\ntime = 6\nnumber = 0\nwhile time != 0:\n    number  += 1\n    seedword = word[number]\n    fileOut.write(generateText(model, startString=seedword))\n    print(time, \" / \", \"6\", \" - \", seedword)\n    time = time-1\n\n# Use sequencing for word variation\n\n# for word in prompts:\n    #filename = word + \".txt\"\n    #textOut = open(filename, \"w\")\n    #textOut.write(generateText(model, startString=word))\n    # textOut.close()\n    # print(\"written\")\n\n#print(generateText(model, startString=\"Romeo: \"))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-29T12:05:30.883306Z","iopub.execute_input":"2021-12-29T12:05:30.883564Z","iopub.status.idle":"2021-12-29T12:28:00.754289Z","shell.execute_reply.started":"2021-12-29T12:05:30.883535Z","shell.execute_reply":"2021-12-29T12:28:00.753556Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
