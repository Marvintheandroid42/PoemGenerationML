{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow._api.v2 import data\n# remember to change the file in which the training is stored in '\ntext = open(\"../input/dickenson-dataset/Dickensondata.txt\", 'rb').read().decode(\n    encoding='UTF-8', errors='ignore')\nvocab = sorted(set(text))\nprint(vocab)\nchar2idx = {u: i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\ntextAsInt = np.array([char2idx[char] for char in text])\nseqLength = 100\nexamplesPerEpoch = len(text)//(seqLength+1)\ncharDataset = tf.data.Dataset.from_tensor_slices(textAsInt)\nsequences = charDataset.batch(seqLength+1, drop_remainder=True)\n\ndef splitInputTarget(chunk):\n    inputText = chunk[:-1]\n    targetText = chunk[1:]\n    return inputText, targetText\n\ndataset = sequences.map(splitInputTarget)\n\nBATCH_SIZE = 28\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\nvocabSize = len(vocab)\nembeddingDim = 264\nrnnUnits = 1024\n\ndef buildModel(vocabSize, embeddingDim, rnnUnits, batchSize):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(\n            vocabSize, embeddingDim, batch_input_shape=[batchSize, None]),\n        tf.keras.layers.GRU(rnnUnits, return_sequences=True,\n                            stateful=True, recurrent_initializer=\"glorot_uniform\"),\n        tf.keras.layers.LSTM(rnnUnits, return_sequences=True,\n                            stateful=True, recurrent_initializer=\"glorot_uniform\"),\n        tf.keras.layers.LSTM(rnnUnits, return_sequences=True,\n                            stateful=True, recurrent_initializer=\"glorot_uniform\"),\n        tf.keras.layers.GRU(rnnUnits, return_sequences=True,\n                            stateful=True, recurrent_initializer=\"glorot_uniform\"),\n        tf.keras.layers.Dense(vocabSize)\n    ])\n    return model\n\nmodel = buildModel(vocabSize=len(vocab), embeddingDim=embeddingDim,\n                   rnnUnits=rnnUnits, batchSize=BATCH_SIZE)\n\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nmodel.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])\n\ncheckpointDir = \"./kaggle/working/training_checkpoint_dickensonParametersChanged1\"\ncheckpointPrefix = os.path.join(checkpointDir, \"chkpt_{epoch}\")\ncheckpointCallback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpointPrefix, save_weights_only=True, monitor='val_accuracy')\n\nEPOCHS = 50\n\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpointCallback])","metadata":{"execution":{"iopub.status.busy":"2021-12-29T16:41:59.197897Z","iopub.execute_input":"2021-12-29T16:41:59.198151Z","iopub.status.idle":"2021-12-29T16:49:29.176068Z","shell.execute_reply.started":"2021-12-29T16:41:59.198120Z","shell.execute_reply":"2021-12-29T16:49:29.175310Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"['\\n', '\\r', ' ', '!', '\"', \"'\", ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\nEpoch 1/50\n43/43 [==============================] - 9s 136ms/step - loss: 3.5201 - accuracy: 0.1415\nEpoch 2/50\n43/43 [==============================] - 6s 137ms/step - loss: 3.1275 - accuracy: 0.1959\nEpoch 3/50\n43/43 [==============================] - 6s 136ms/step - loss: 2.6472 - accuracy: 0.2867\nEpoch 4/50\n43/43 [==============================] - 6s 136ms/step - loss: 2.2881 - accuracy: 0.3553\nEpoch 5/50\n43/43 [==============================] - 6s 137ms/step - loss: 2.1453 - accuracy: 0.3867\nEpoch 6/50\n43/43 [==============================] - 6s 137ms/step - loss: 2.0589 - accuracy: 0.4057\nEpoch 7/50\n43/43 [==============================] - 6s 137ms/step - loss: 1.9678 - accuracy: 0.4314\nEpoch 8/50\n43/43 [==============================] - 6s 136ms/step - loss: 1.8726 - accuracy: 0.4560\nEpoch 9/50\n43/43 [==============================] - 6s 136ms/step - loss: 1.7818 - accuracy: 0.4812\nEpoch 10/50\n43/43 [==============================] - 6s 137ms/step - loss: 1.6940 - accuracy: 0.5036\nEpoch 11/50\n43/43 [==============================] - 6s 137ms/step - loss: 1.6047 - accuracy: 0.5257\nEpoch 12/50\n43/43 [==============================] - 6s 137ms/step - loss: 1.5203 - accuracy: 0.5504\nEpoch 13/50\n43/43 [==============================] - 6s 136ms/step - loss: 1.4240 - accuracy: 0.5764\nEpoch 14/50\n43/43 [==============================] - 6s 136ms/step - loss: 1.3215 - accuracy: 0.6073\nEpoch 15/50\n43/43 [==============================] - 6s 136ms/step - loss: 1.2108 - accuracy: 0.6386\nEpoch 16/50\n43/43 [==============================] - 6s 137ms/step - loss: 1.0976 - accuracy: 0.6766\nEpoch 17/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.9674 - accuracy: 0.7204\nEpoch 18/50\n43/43 [==============================] - 6s 137ms/step - loss: 0.8438 - accuracy: 0.7628\nEpoch 19/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.7250 - accuracy: 0.8049\nEpoch 20/50\n43/43 [==============================] - 6s 137ms/step - loss: 0.6082 - accuracy: 0.8456\nEpoch 21/50\n43/43 [==============================] - 6s 137ms/step - loss: 0.5129 - accuracy: 0.8780\nEpoch 22/50\n43/43 [==============================] - 6s 137ms/step - loss: 0.4266 - accuracy: 0.9045\nEpoch 23/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.3628 - accuracy: 0.9221\nEpoch 24/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.3234 - accuracy: 0.9335\nEpoch 25/50\n43/43 [==============================] - 6s 137ms/step - loss: 0.2905 - accuracy: 0.9407\nEpoch 26/50\n43/43 [==============================] - 6s 137ms/step - loss: 0.2677 - accuracy: 0.9458\nEpoch 27/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.2459 - accuracy: 0.9517\nEpoch 28/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.2317 - accuracy: 0.9536\nEpoch 29/50\n43/43 [==============================] - 6s 137ms/step - loss: 0.2208 - accuracy: 0.9560\nEpoch 30/50\n43/43 [==============================] - 6s 137ms/step - loss: 0.2128 - accuracy: 0.9575\nEpoch 31/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.2011 - accuracy: 0.9592\nEpoch 32/50\n43/43 [==============================] - 6s 137ms/step - loss: 0.1950 - accuracy: 0.9600\nEpoch 33/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.1930 - accuracy: 0.9609\nEpoch 34/50\n43/43 [==============================] - 6s 137ms/step - loss: 0.1878 - accuracy: 0.9613\nEpoch 35/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.1839 - accuracy: 0.9624\nEpoch 36/50\n43/43 [==============================] - 6s 137ms/step - loss: 0.1770 - accuracy: 0.9635\nEpoch 37/50\n43/43 [==============================] - 6s 137ms/step - loss: 0.1740 - accuracy: 0.9640\nEpoch 38/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.1706 - accuracy: 0.9641\nEpoch 39/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.1705 - accuracy: 0.9647\nEpoch 40/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.1658 - accuracy: 0.9652\nEpoch 41/50\n43/43 [==============================] - 6s 137ms/step - loss: 0.1643 - accuracy: 0.9657\nEpoch 42/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.1636 - accuracy: 0.9655\nEpoch 43/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.1559 - accuracy: 0.9668\nEpoch 44/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.1566 - accuracy: 0.9675\nEpoch 45/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.1544 - accuracy: 0.9672\nEpoch 46/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.1532 - accuracy: 0.9673\nEpoch 47/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.1513 - accuracy: 0.9682\nEpoch 48/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.1497 - accuracy: 0.9681\nEpoch 49/50\n43/43 [==============================] - 6s 135ms/step - loss: 0.1464 - accuracy: 0.9689\nEpoch 50/50\n43/43 [==============================] - 6s 136ms/step - loss: 0.1424 - accuracy: 0.9687\n","output_type":"stream"}]},{"cell_type":"code","source":"model = buildModel(vocabSize, embeddingDim, rnnUnits, batchSize=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpointDir))\n\nmodel.build(tf.TensorShape([1, None]))\n\nmodel.summary()\n\n\ndef generateText(model, startString):\n    # of charectars generated\n    numGenerate = 20000\n    # convert start string into idx\n    inputRep = [char2idx[s] for s in startString]\n    inputRep = tf.expand_dims(inputRep, 0)\n\n    txtGenerated = []\n    # Handles randomness through a scale factor (smaller means predictable(increase for more randomness))\n    tempreture = 1.0\n    model.reset_states()\n    for i in range(numGenerate):\n        predictions = model(inputRep)\n        predictions = tf.squeeze(predictions, 0)\n        predictions = predictions/tempreture\n        predictedID = tf.random.categorical(\n            predictions, num_samples=1)[-1, 0].numpy()\n\n        inputRep = tf.expand_dims([predictedID], 0)\n        txtGenerated.append(idx2char[predictedID])\n    return (startString + \"\".join(txtGenerated))\n\n\nprint(\"PASSED\")\n\nword = [\"the\", \"Paradise\", \"Solitude\", \"Death\", \"The\", \"Love\",\"Family\", \"Young\" ]\nfileOut = open(\"./kaggle/working/DickensonGen1.txt\", \"w\")\ntime = 6\nnumber = 0\nwhile time != 0:\n    number  += 1\n    seedword = word[number]\n    fileOut.write(generateText(model, startString=seedword))\n    print(time, \" / \", \"6\", \" - \", seedword)\n    time = time-1","metadata":{"execution":{"iopub.status.busy":"2021-12-29T16:53:17.495476Z","iopub.execute_input":"2021-12-29T16:53:17.496102Z","iopub.status.idle":"2021-12-29T17:16:01.935723Z","shell.execute_reply.started":"2021-12-29T16:53:17.496064Z","shell.execute_reply":"2021-12-29T17:16:01.933941Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (1, None, 264)            16632     \n_________________________________________________________________\ngru_4 (GRU)                  (1, None, 1024)           3962880   \n_________________________________________________________________\nlstm_4 (LSTM)                (1, None, 1024)           8392704   \n_________________________________________________________________\nlstm_5 (LSTM)                (1, None, 1024)           8392704   \n_________________________________________________________________\ngru_5 (GRU)                  (1, None, 1024)           6297600   \n_________________________________________________________________\ndense_2 (Dense)              (1, None, 63)             64575     \n=================================================================\nTotal params: 27,127,095\nTrainable params: 27,127,095\nNon-trainable params: 0\n_________________________________________________________________\nPASSED\n6  /  6  -  Paradise\n5  /  6  -  Solitude\n4  /  6  -  Death\n3  /  6  -  The\n2  /  6  -  Love\n1  /  6  -  Family\n","output_type":"stream"}]}]}
