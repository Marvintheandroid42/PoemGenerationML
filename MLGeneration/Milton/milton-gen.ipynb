{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-29T13:54:13.160793Z","iopub.execute_input":"2021-12-29T13:54:13.161222Z","iopub.status.idle":"2021-12-29T13:54:13.175322Z","shell.execute_reply.started":"2021-12-29T13:54:13.161178Z","shell.execute_reply":"2021-12-29T13:54:13.174496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow._api.v2 import data\n\n# remember to change the file in which the training is stored in '\n\ntext = open(\"../input/MiltonPoemLines/otherPoemStorage.txt\", 'rb').read().decode(\n    encoding='UTF-8', errors='ignore')\n\nvocab = sorted(set(text))\n\nprint(vocab)\n\nchar2idx = {u: i for i, u in enumerate(vocab)}\n\nidx2char = np.array(vocab)\n\ntextAsInt = np.array([char2idx[char] for char in text])\n\nseqLength = 100\nexamplesPerEpoch = len(text)//(seqLength+1)\n\ncharDataset = tf.data.Dataset.from_tensor_slices(textAsInt)\n\nsequences = charDataset.batch(seqLength+1, drop_remainder=True)\n\n\ndef splitInputTarget(chunk):\n    inputText = chunk[:-1]\n    targetText = chunk[1:]\n    return inputText, targetText\n\n\ndataset = sequences.map(splitInputTarget)\n\nBATCH_SIZE = 128\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\nvocabSize = len(vocab)\nembeddingDim = 264\nrnnUnits = 1024\n\n\ndef buildModel(vocabSize, embeddingDim, rnnUnits, batchSize):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(\n            vocabSize, embeddingDim, batch_input_shape=[batchSize, None]),\n        tf.keras.layers.GRU(rnnUnits, return_sequences=True,\n                            stateful=True, recurrent_initializer=\"glorot_uniform\"),\n        tf.keras.layers.LSTM(rnnUnits, return_sequences=True,\n                            stateful=True, recurrent_initializer=\"glorot_uniform\"),\n        tf.keras.layers.LSTM(rnnUnits, return_sequences=True,\n                            stateful=True, recurrent_initializer=\"glorot_uniform\"),\n        tf.keras.layers.GRU(rnnUnits, return_sequences=True,\n                            stateful=True, recurrent_initializer=\"glorot_uniform\"),\n        tf.keras.layers.Dense(vocabSize)\n    ])\n    return model\n\n\nmodel = buildModel(vocabSize=len(vocab), embeddingDim=embeddingDim,\n                   rnnUnits=rnnUnits, batchSize=BATCH_SIZE)\n\n\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\n\nmodel.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])\n\ncheckpointDir = \"./kaggle/working/training_checkpoint_miltonParametersChanged\"\ncheckpointPrefix = os.path.join(checkpointDir, \"chkpt_{epoch}\")\ncheckpointCallback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpointPrefix, save_weights_only=True, monitor='val_accuracy')\n\nEPOCHS = 50\n\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpointCallback])","metadata":{"execution":{"iopub.status.busy":"2021-12-29T13:54:33.660820Z","iopub.execute_input":"2021-12-29T13:54:33.661218Z","iopub.status.idle":"2021-12-29T14:11:18.148718Z","shell.execute_reply.started":"2021-12-29T13:54:33.661185Z","shell.execute_reply":"2021-12-29T14:11:18.147930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = buildModel(vocabSize, embeddingDim, rnnUnits, batchSize=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpointDir))\n\nmodel.build(tf.TensorShape([1, None]))\n\nmodel.summary()\n\n\ndef generateText(model, startString):\n    # of charectars generated\n    numGenerate = 20000\n    # convert start string into idx\n    inputRep = [char2idx[s] for s in startString]\n    inputRep = tf.expand_dims(inputRep, 0)\n\n    txtGenerated = []\n    # Handles randomness through a scale factor (smaller means predictable(increase for more randomness))\n    tempreture = 1.0\n    model.reset_states()\n    for i in range(numGenerate):\n        predictions = model(inputRep)\n        predictions = tf.squeeze(predictions, 0)\n        predictions = predictions/tempreture\n        predictedID = tf.random.categorical(\n            predictions, num_samples=1)[-1, 0].numpy()\n\n        inputRep = tf.expand_dims([predictedID], 0)\n        txtGenerated.append(idx2char[predictedID])\n    return (startString + \"\".join(txtGenerated))\n\n\nprint(\"PASSED\")\n\nword = [\"the\", \"Paradise\", \"Solitude\", \"Death\", \"The\", \"Love\",\"Family\", \"Young\" ]\nfileOut = open(\"./kaggle/working/MiltonGen1.txt\", \"w\")\ntime = 6\nnumber = 0\nwhile time != 0:\n    number  += 1\n    seedword = word[number]\n    fileOut.write(generateText(model, startString=seedword))\n    print(time, \" / \", \"6\", \" - \", seedword)\n    time = time-1","metadata":{"execution":{"iopub.status.busy":"2021-12-29T14:15:19.904898Z","iopub.execute_input":"2021-12-29T14:15:19.905807Z","iopub.status.idle":"2021-12-29T14:40:05.546289Z","shell.execute_reply.started":"2021-12-29T14:15:19.905773Z","shell.execute_reply":"2021-12-29T14:40:05.544183Z"},"trusted":true},"execution_count":null,"outputs":[]}]}